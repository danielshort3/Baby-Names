{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5000ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import zipfile\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from langdetect import DetectorFactory, detect\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767a8a8",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00f756d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize\n",
    "DetectorFactory.seed = 0\n",
    "western_lang = [\"de\", \"en\", \"es\", \"it\", \"el\", \"nl\", \"da\"]\n",
    "\n",
    "# Create a unique folder to extract files\n",
    "extracted_folder = \"data/namesbystate_extracted\"\n",
    "if not os.path.exists(extracted_folder):\n",
    "    with zipfile.ZipFile(\"data/namesbystate.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_folder)\n",
    "\n",
    "# Read Colorado-specific file\n",
    "co_df = pd.read_csv(os.path.join(extracted_folder, \"STATE.CO.txt\"), header=None, names=[\"State\", \"Sex\", \"Year\", \"Name\", \"Count\"])\n",
    "\n",
    "# Aggregate for total count and most popular year in Colorado\n",
    "co_aggregated = co_df.groupby(['Name', 'Sex']).agg({\n",
    "    'Count': 'sum',\n",
    "    'Year': lambda x: x.value_counts().idxmax()\n",
    "}).reset_index()\n",
    "co_aggregated.rename(columns={'Count': 'CO_Total_Count', 'Year': 'CO_Most_Popular_Year'}, inplace=True)\n",
    "\n",
    "# Extracting zip file to a folder named \"Extracted\"\n",
    "extracted_folder = \"data/Extracted\"\n",
    "\n",
    "# Check if the folder already exists\n",
    "if not os.path.exists(extracted_folder):\n",
    "    with zipfile.ZipFile(\"data/names.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_folder)\n",
    "\n",
    "# Reading and merging all txt files\n",
    "all_files = glob.glob(os.path.join(extracted_folder, \"yob*.txt\"))  # Adjusted path\n",
    "df_list = []\n",
    "\n",
    "for filename in all_files:\n",
    "    year = filename[-8:-4]  # Extracting the year from filename\n",
    "    df = pd.read_csv(filename, names=[\"Name\", \"Sex\", \"Count\"])\n",
    "    df['Year'] = int(year)  # Adding year column\n",
    "    df_list.append(df)\n",
    "    \n",
    "# Concatenating all dataframes\n",
    "all_names = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Grouping by 'Name' and 'Sex', then aggregating\n",
    "aggregated_names = all_names.groupby(['Name', 'Sex']).agg({\n",
    "    'Count': 'sum',\n",
    "    'Year': lambda x: x.value_counts().idxmax()  # Most frequent year\n",
    "}).reset_index()\n",
    "aggregated_names\n",
    "\n",
    "# Renaming columns for clarity\n",
    "aggregated_names.rename(columns={'Count': 'Total_Count', 'Year': 'Most_Popular_Year'}, inplace=True)\n",
    "aggregated_names['Most_Popular_Year'] = aggregated_names['Most_Popular_Year'].fillna(0).astype(int)\n",
    "aggregated_names.sort_values(by='Total_Count', ascending=False)\n",
    "\n",
    "# Getting the most recent year in CO data\n",
    "most_recent_year_CO = co_df['Year'].max()\n",
    "\n",
    "# Filtering the most recent year data and aggregating counts by Name and Sex\n",
    "co_recent_counts = co_df[co_df['Year'] == most_recent_year_CO].groupby(['Name', 'Sex'])['Count'].sum().reset_index()\n",
    "co_recent_counts.rename(columns={'Count': 'Count_CO_Recent'}, inplace=True)\n",
    "co_recent_counts.sort_values(by='Count_CO_Recent', ascending=False)\n",
    "\n",
    "# Getting the most recent year in the original dataset\n",
    "most_recent_year_all = aggregated_names['Most_Popular_Year'].max()\n",
    "\n",
    "# Aggregated names should have a 'Year' column populated to filter\n",
    "all_recent_counts = (all_names[all_names['Year'] == most_recent_year_all]\n",
    "                     .rename(columns={'Count': 'Count_All_Recent'})\n",
    "                     .drop(columns='Year'))\n",
    "all_recent_counts.sort_values(by='Count_All_Recent', ascending=False)\n",
    "\n",
    "# Adding counts from the most recent year in CO\n",
    "aggregated_names = pd.merge(aggregated_names, co_recent_counts, on=['Name', 'Sex'], how='left')\n",
    "\n",
    "# Adding counts from the most recent year in the original dataset\n",
    "aggregated_names = pd.merge(aggregated_names, all_recent_counts[['Name', 'Sex', 'Count_All_Recent']], on=['Name', 'Sex'], how='left')\n",
    "\n",
    "# Filling NA\n",
    "aggregated_names['Count_CO_Recent'] = aggregated_names['Count_CO_Recent'].fillna(0).astype(int)\n",
    "aggregated_names['Count_All_Recent'] = aggregated_names['Count_All_Recent'].fillna(0).astype(int)\n",
    "\n",
    "# Reading the names rated by wife\n",
    "rated_boys = pd.read_csv(\"data/baby_name_boy.txt\", names=[\"Name\", \"Rating\", \"Sex\"])\n",
    "rated_girls = pd.read_csv(\"data/baby_name_girl.txt\", names=[\"Name\", \"Rating\", \"Sex\"])\n",
    "rated_names = pd.concat([rated_boys, rated_girls], ignore_index=True)\n",
    "rated_names = pd.merge(rated_names, aggregated_names[['Name', 'Sex', 'Total_Count', 'Most_Popular_Year']], \n",
    "                       on=['Name', 'Sex'], how='left')\n",
    "rated_names['Total_Count'] = rated_names['Total_Count'].fillna(0).astype(int)\n",
    "rated_names['Most_Popular_Year'] = rated_names['Most_Popular_Year'].fillna(0).astype(int)\n",
    "\n",
    "# Merge Colorado-specific data into your existing dataframes\n",
    "aggregated_names = pd.merge(aggregated_names, co_aggregated, on=['Name', 'Sex'], how='left')\n",
    "rated_names = pd.merge(rated_names, co_aggregated, on=['Name', 'Sex'], how='left')\n",
    "\n",
    "rated_names = pd.merge(rated_names, co_recent_counts, on=['Name', 'Sex'], how='left')\n",
    "rated_names = pd.merge(rated_names, all_recent_counts, on=['Name', 'Sex'], how='left')\n",
    "\n",
    "# Fill NA values for the new columns\n",
    "for df in [aggregated_names, rated_names]:\n",
    "    df.loc[df['CO_Most_Popular_Year'].isna(), 'CO_Most_Popular_Year'] = df['Most_Popular_Year']\n",
    "    df['Count_All_Recent'] = df['Count_All_Recent'].fillna(0).astype(int)\n",
    "    df['Count_CO_Recent'] = df['Count_CO_Recent'].fillna(0).astype(int)\n",
    "    df['CO_Total_Count'] = df['CO_Total_Count'].fillna(0).astype(int)\n",
    "    df.loc[df['CO_Most_Popular_Year'].isna(), 'CO_Most_Popular_Year'] = df['Most_Popular_Year']\n",
    "    df['CO_Most_Popular_Year'] = df['CO_Most_Popular_Year'].astype(int)\n",
    "        \n",
    "aggregated_names = aggregated_names[['Name', 'Sex', 'Total_Count', 'Count_All_Recent', 'Most_Popular_Year', 'CO_Total_Count', 'Count_CO_Recent', 'CO_Most_Popular_Year']]\n",
    "rated_names = rated_names[['Name', 'Sex', 'Total_Count', 'Count_All_Recent', 'Most_Popular_Year', 'CO_Total_Count', 'Count_CO_Recent', 'CO_Most_Popular_Year', 'Rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da809b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save DataFrame to a pickle file\n",
    "def save_to_pickle(df, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "\n",
    "# Function to load DataFrame from a pickle file\n",
    "def load_from_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def syllable_count(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def entropy(string):\n",
    "    p, lns = Counter(string), float(len(string))\n",
    "    return -sum(count/lns * math.log(count/lns, 2) for count in p.values())\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # Existing features\n",
    "    df['Length'] = df['Name'].progress_apply(len)\n",
    "    df['Vowels'] = df['Name'].progress_apply(lambda x: sum(1 for char in x.lower() if char in 'aeiou'))\n",
    "    df['Consonants'] = df['Length'] - df['Vowels']\n",
    "    df['Sex'] = df['Sex'].progress_apply(lambda x: 1 if x == 'M' else 0)\n",
    "    df['Has_Repeating'] = df['Name'].progress_apply(lambda x: 1 if max(Counter(x).values()) > 1 else 0)\n",
    "    df['Ends_with_Vowel'] = df['Name'].progress_apply(lambda x: 1 if x[-1].lower() in 'aeiou' else 0)\n",
    "    df['Starts_with_Vowel'] = df['Name'].progress_apply(lambda x: 1 if x[0].lower() in 'aeiou' else 0)\n",
    "    df['Syllable_Count'] = df['Name'].progress_apply(syllable_count)\n",
    "    df['Name_Entropy'] = df['Name'].progress_apply(entropy)\n",
    "    df['Language_Root'] = df['Name'].progress_apply(lambda x: 1 if detect(x) in western_lang else 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff2f2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from saved file.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6383e6ecf0ac40ba9b6755dc5329c5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfea675dab84ac995ffdfb4594f7bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbedc937d4548fa8e03e3a714a1548f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c917051e9784b3ba053045b8a3cb07b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abf96cc6af54252bbc4a72f6fc81541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd9ac0634ca4e109e8f62bdbf737edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d483d803da8645689c5407936fd39f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c4493ca4a14fd4a0f60e1b2a42fb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c883fcdba4e4aa9ae236a311f4e3194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the name of the saved file\n",
    "saved_filename = 'data/all_names.pkl'\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    # Check if the saved file exists\n",
    "    if os.path.exists(saved_filename):\n",
    "        print(\"Loading data from saved file.\")\n",
    "        all_names = load_from_pickle(saved_filename)\n",
    "    else:\n",
    "        print(\"File not found. Running the feature engineering process.\")\n",
    "\n",
    "        # Run your existing code to create 'all_names'\n",
    "        all_names = feature_engineering(aggregated_names)\n",
    "\n",
    "        # Save 'all_names' to a file for future use\n",
    "        save_to_pickle(all_names, saved_filename)\n",
    "    rated_names = feature_engineering(rated_names)\n",
    "\n",
    "feature_cols = [col for col in all_names.columns if col != 'Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22149bc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = rated_names[feature_cols].values\n",
    "y = rated_names[\"Rating\"].values\n",
    "y = y - 1\n",
    "\n",
    "class_names = np.sort(pd.DataFrame(y)[0].unique())\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Set up cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b39001",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e3a48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percentage_conf_matrix(conf_matrix, class_labels):\n",
    "    row_sums = conf_matrix.sum(axis=1)\n",
    "    norm_conf_matrix = conf_matrix / row_sums[:, np.newaxis]\n",
    "    percentage_matrix = np.round(norm_conf_matrix * 100, 2)\n",
    "    \n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(percentage_matrix, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    \n",
    "    plt.title(\"Confusion Matrix (Percentage)\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a4a165",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49affa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sd205521/anaconda3/envs/rapids-23.12/lib/python3.10/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)))\n",
    "\n",
    "# Set up hyperparameter grid for search\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 250],\n",
    "    'learning_rate': [.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "xgb_grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=xgb_param_grid, \n",
    "                           cv=cv, \n",
    "                           scoring='f1_weighted', \n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "xgb_best_params = xgb_grid_search.best_params_\n",
    "print(f\"Best xgb parameters: {xgb_best_params}\")\n",
    "\n",
    "# Evaluate the model with best parameters\n",
    "xgb_best_model = xgb_grid_search.best_estimator_\n",
    "xgb_y_pred = xgb_best_model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_y_pred)\n",
    "xgb_f1 = f1_score(y_test, xgb_y_pred, average='macro')\n",
    "xgb_conf_matrix = confusion_matrix(y_test, xgb_y_pred)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy}\")\n",
    "print(f\"XGBoost F1 Score: {xgb_f1}\")\n",
    "plot_percentage_conf_matrix(xgb_conf_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ed1de4",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cd396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest model\n",
    "rf_model = RandomForestClassifier(class_weight='balanced')\n",
    "\n",
    "# Set up hyperparameter grid for search\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "rf_grid_search = GridSearchCV(estimator=rf_model, \n",
    "                           param_grid=rf_param_grid, \n",
    "                           cv=cv, \n",
    "                           scoring='f1_weighted', \n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "rf_best_params = rf_grid_search.best_params_\n",
    "print(f\"Best rf parameters: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model with best parameters\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "rf_y_pred = rf_best_model.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "rf_f1 = f1_score(y_test, rf_y_pred, average='weighted')\n",
    "rf_conf_matrix = confusion_matrix(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "print(f\"Random Forest F1 Score: {rf_f1}\")\n",
    "plot_percentage_conf_matrix(rf_conf_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9067426",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0616edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 10 \n",
    "svm_pca = PCA(n_components=n_components)\n",
    "svm_X_train_pca = svm_pca.fit_transform(X_train)\n",
    "svm_X_test_pca = svm_pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVM model\n",
    "svm_model = SVC(kernel='rbf', class_weight='balanced')\n",
    "\n",
    "# Set up hyperparameter grid for search\n",
    "svm_param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "svm_grid_search = GridSearchCV(estimator=svm_model, \n",
    "                           param_grid=svm_param_grid, \n",
    "                           cv=cv, \n",
    "                           scoring='f1_weighted', \n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "svm_grid_search.fit(svm_X_train_pca, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "svm_best_params = svm_grid_search.best_params_\n",
    "print(f\"Best svm parameters: {svm_grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model with best parameters\n",
    "svm_best_model = svm_grid_search.best_estimator_\n",
    "svm_y_pred = svm_best_model.predict(svm_X_test_pca)\n",
    "\n",
    "# Evaluation metrics\n",
    "svm_accuracy = accuracy_score(y_test, svm_y_pred)\n",
    "svm_f1 = f1_score(y_test, svm_y_pred, average='weighted')\n",
    "svm_conf_matrix = confusion_matrix(y_test, svm_y_pred)\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "print(f\"SVM F1 Score: {svm_f1}\")\n",
    "plot_percentage_conf_matrix(svm_conf_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b402db",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "n_components = 10 \n",
    "knn_pca = PCA(n_components=n_components)\n",
    "knn_X_train_pca = knn_pca.fit_transform(X_train)\n",
    "knn_X_test_pca = knn_pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcce784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Set up hyperparameter grid for search\n",
    "knn_param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_grid_search = GridSearchCV(estimator=knn_model, \n",
    "                           param_grid=knn_param_grid, \n",
    "                           cv= cv, \n",
    "                           scoring='f1_weighted', \n",
    "                           verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "knn_grid_search.fit(knn_X_train_pca, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "knn_best_params = knn_grid_search.best_params_\n",
    "print(f\"Best svm parameters: {knn_grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the model with best parameters\n",
    "knn_best_model = knn_grid_search.best_estimator_\n",
    "knn_y_pred = knn_best_model.predict(knn_X_test_pca)\n",
    "\n",
    "# Evaluation metrics\n",
    "knn_y_pred = knn_grid_search.predict(knn_X_test_pca)\n",
    "knn_accuracy = accuracy_score(y_test, knn_y_pred)\n",
    "knn_f1 = f1_score(y_test, knn_y_pred, average='weighted')\n",
    "knn_conf_matrix = confusion_matrix(y_test, knn_y_pred)\n",
    "\n",
    "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
    "print(f\"KNN F1 Score: {knn_f1}\")\n",
    "plot_percentage_conf_matrix(knn_conf_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44641149",
   "metadata": {},
   "source": [
    "## Tensorflow Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eac5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class SingleLineProgress(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metrics_str = \" - \".join([f\"{key}: {val:.4f}\" for key, val in logs.items()])\n",
    "        clear_output(wait=True)\n",
    "        display(f\"Epoch {epoch + 1}: {metrics_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aba392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define function for creating the neural network model\n",
    "def create_tf_model(neuron1, neuron2):\n",
    "    tf_model = Sequential([\n",
    "        Dense(neuron1, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(neuron2, activation='relu'),\n",
    "        Dense(10, activation='softmax')  # Assuming 10 classes\n",
    "    ])\n",
    "    tf_model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=[F1Score(average='weighted')])\n",
    "    return tf_model\n",
    "\n",
    "# One-hot encoding for target labels\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Set up hyperparameter grid for search\n",
    "neuron_combinations = [(16, 16), (32, 16), (32, 32), (64, 32), (64, 64), (64, 128), (128, 128)]\n",
    "\n",
    "best_f1 = 0\n",
    "best_neuron_combo = None\n",
    "best_tf_model = None\n",
    "\n",
    "for neuron1, neuron2 in neuron_combinations:\n",
    "    # Create the neural network model\n",
    "    tf_model = create_tf_model(neuron1, neuron2)\n",
    "\n",
    "    # Fit the model\n",
    "    tf_model.fit(X_train, \n",
    "                 y_train_one_hot, \n",
    "                 epochs=100, \n",
    "                 batch_size=16, \n",
    "                 verbose=0)  # Disable the default progress bar\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    tf_y_pred_probs = tf_model.predict(X_test, verbose=0)\n",
    "    tf_y_pred = np.argmax(tf_y_pred_probs, axis=1)\n",
    "    tf_f1 = f1_score(y_test, tf_y_pred, average='weighted')\n",
    "    \n",
    "    if tf_f1 > best_f1:\n",
    "        best_f1 = tf_f1\n",
    "        best_neuron_combo = (neuron1, neuron2)\n",
    "        best_tf_model = tf_model\n",
    "\n",
    "# Use the best model found\n",
    "print(f\"Best neuron combination: {best_neuron_combo} with F1 Score: {best_f1}\")\n",
    "\n",
    "tf_y_pred_probs = best_tf_model.predict(X_test, verbose=0)\n",
    "tf_y_pred = np.argmax(tf_y_pred_probs, axis=1)\n",
    "tf_accuracy = accuracy_score(y_test, tf_y_pred)\n",
    "tf_conf_matrix = confusion_matrix(y_test, tf_y_pred)\n",
    "\n",
    "print(f\"Tensorflow Accuracy: {tf_accuracy}\")\n",
    "print(f\"Tensorflow F1 Score: {best_f1}\")\n",
    "plot_percentage_conf_matrix(tf_conf_matrix, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905c40d",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfcda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Model': ['XGBoost', 'Random Forest', 'Support Vector Machines', 'K-Nearest Neighbors', 'Tensorflow'],\n",
    "    'F1_Score': [xgb_f1, rf_f1, svm_f1, knn_f1, tf_f1],\n",
    "    'Accuracy': [xgb_accuracy, rf_accuracy, svm_accuracy, knn_accuracy, tf_accuracy]\n",
    "}\n",
    "\n",
    "pd.DataFrame(data).sort_values(by='F1_Score', ascending=False).plot(kind='bar', x='Model', title=\"Accuracy and F1 Scores for Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d9ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "all_names_features = all_names[feature_cols].values  \n",
    "all_names_scaled = scaler.transform(all_names_features) \n",
    "\n",
    "# Prediction using trained models\n",
    "rf_predictions_all_names = rf_best_model.predict(all_names_scaled)\n",
    "xgb_predictions_all_names = xgb_best_model.predict(all_names_scaled)\n",
    "svm_predictions_all_names = svm_best_model.predict(svm_pca.transform(all_names_scaled))\n",
    "knn_predictions_all_names = knn_best_model.predict(knn_pca.transform(all_names_scaled))\n",
    "tf_all_names_pred_probs  = tf_model.predict(all_names_scaled, verbose=0)\n",
    "tf_predictions_all_names = np.argmax(tf_all_names_pred_probs, axis=1)\n",
    "\n",
    "# Add these predictions back to the all_names dataframe\n",
    "all_names['RF_Predicted_Rating'] = rf_predictions_all_names\n",
    "all_names['XGB_Predicted_Rating'] = xgb_predictions_all_names\n",
    "all_names['SVM_Predicted_Rating'] = svm_predictions_all_names\n",
    "all_names['KNN_Predicted_Rating'] = knn_predictions_all_names\n",
    "all_names['Tensorflow_Predicted_Rating'] = tf_predictions_all_names\n",
    "all_names['Average_Predicted_Rating'] = all_names[['RF_Predicted_Rating', 'XGB_Predicted_Rating', 'SVM_Predicted_Rating', 'KNN_Predicted_Rating', 'Tensorflow_Predicted_Rating']].mean(axis=1).round(1)\n",
    "\n",
    "# Add 1 back to predictions to scale between 1 and 10\n",
    "columns_to_update = ['RF_Predicted_Rating', 'XGB_Predicted_Rating', 'SVM_Predicted_Rating', \n",
    "                     'KNN_Predicted_Rating', 'Tensorflow_Predicted_Rating', 'Average_Predicted_Rating']\n",
    "all_names[columns_to_update] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df64003",
   "metadata": {},
   "source": [
    "# Find top names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c835a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighted rows are names already submitted by my wife\n",
    "def highlight_rows(row):\n",
    "    if row['Name'] in rated_names['Name'].values:\n",
    "        return ['background-color: lightyellow']*row.shape[0]\n",
    "    else:\n",
    "        return ['background-color: white']*row.shape[0]\n",
    "\n",
    "highlighted_df = (all_names[all_names['Sex']==1]\n",
    "                  .sort_values(by='Average_Predicted_Rating', ascending=False)\n",
    "                  .reset_index(drop=True)[['Name', \n",
    "                                           'RF_Predicted_Rating', \n",
    "                                           'XGB_Predicted_Rating', \n",
    "                                           'SVM_Predicted_Rating', \n",
    "                                           'KNN_Predicted_Rating', \n",
    "                                           'Tensorflow_Predicted_Rating', \n",
    "                                           'Average_Predicted_Rating']]\n",
    "                  .head(50))\n",
    "\n",
    "highlighted_df.style.apply(highlight_rows, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbddb5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
